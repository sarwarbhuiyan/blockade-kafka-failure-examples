containers:
  zk1:
    image: zookeeper:3.4.9
    hostname: zk1
    container_name: zk1
    environment: { "ZOO_MY_ID": 1, "ZOO_PORT": 2181, "ZOO_SERVERS": "server.1=0.0.0.0:2888:3888" }
    # uncomment if you want the data to survive resets of the cluster
    #volumes: { "./volumes/zookeeper/01/data/data": "/data", "./volumes/zookeeper/01/datalog": "/datalog" }
    expose: [2181]

  kafka1:
    image: confluentinc/cp-kafka:5.0.0
    hostname: kafka1
    container_name: kafka1
    expose: [9092]
    environment: { "KAFKA_ADVERTISED_LISTENERS": "LISTENER_DOCKER_INTERNAL://kafka1:19092,LISTENER_DOCKER_EXTERNAL://172.17.0.3:9092", 
      "KAFKA_LISTENER_SECURITY_PROTOCOL_MAP": "LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT", 
      "KAFKA_INTER_BROKER_LISTENER_NAME": "LISTENER_DOCKER_INTERNAL", 
      "KAFKA_ZOOKEEPER_CONNECT": "zk1:2181", 
      "KAFKA_BROKER_ID": 1, 
      "KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE": "false",
      "KAFKA_LOG4J_LOGGERS": "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO" }
    # uncomment if you want the data to survive resets of the cluster
    #volumes: { "./volumes/kafka/01/data": "/var/lib/kafka/data" }
    start_delay: 5
    
  kafka2:
    image: confluentinc/cp-kafka:5.0.0
    hostname: kafka2
    container_name: kafka2
    expose: [9092]
    environment: { "KAFKA_ADVERTISED_LISTENERS": "LISTENER_DOCKER_INTERNAL://kafka2:19092,LISTENER_DOCKER_EXTERNAL://172.17.0.4:9092",
      "KAFKA_LISTENER_SECURITY_PROTOCOL_MAP": "LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT", 
      "KAFKA_INTER_BROKER_LISTENER_NAME": "LISTENER_DOCKER_INTERNAL",
      "KAFKA_ZOOKEEPER_CONNECT": "zk1:2181",
      "KAFKA_BROKER_ID": 2,
      "KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE": "false",
      "KAFKA_LOG4J_LOGGERS": "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO" }
    # uncomment if you want the data to survive resets of the cluster
    #volumes: { "./volumes/kafka/02/data": "/var/lib/kafka/data" }
    start_delay: 1

  kafka3:
    image: confluentinc/cp-kafka:5.0.0
    hostname: kafka3
    container_name: kafka3
    expose: [9092]
    environment: { "KAFKA_ADVERTISED_LISTENERS": "LISTENER_DOCKER_INTERNAL://kafka3:19092,LISTENER_DOCKER_EXTERNAL://172.17.0.5:9092",
      "KAFKA_LISTENER_SECURITY_PROTOCOL_MAP": "LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT", 
      "KAFKA_INTER_BROKER_LISTENER_NAME": "LISTENER_DOCKER_INTERNAL",
      "KAFKA_ZOOKEEPER_CONNECT": "zk1:2181",
      "KAFKA_BROKER_ID": 3,
      "KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE": "false",
      "KAFKA_LOG4J_LOGGERS": "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO" }
    # uncomment if you want the data to survive resets of the cluster
    #volumes: { "./volumes/kafka/03/data": "/var/lib/kafka/data" }
    start_delay: 1

network:
  driver: udn
  flaky: 5%
  slow: 100ms 50ms 25% distribution normal